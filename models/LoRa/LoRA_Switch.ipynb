{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T03:37:55.618858Z",
     "start_time": "2026-01-01T03:37:53.097842Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ],
   "id": "9204cff5c92d67d3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dam/miniforge3/envs/poemas/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T03:37:57.881071Z",
     "start_time": "2026-01-01T03:37:57.846061Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Limpiar memoria para Mac (MPS)\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.empty_cache()\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "dtype = torch.float16"
   ],
   "id": "2b6d285a26a5817b",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T03:38:05.306627Z",
     "start_time": "2026-01-01T03:38:05.302299Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Definición de rutas absolutas\n",
    "BASE_MODEL = \"/Users/dam/ESCOM/ML/Poemas/models/BASE_MODEL\"\n",
    "LORA_BASE_PATH = \"/Users/dam/ESCOM/ML/Poemas/models/LoRa\"\n",
    "\n",
    "# Adaptadores LoRA con su ruta completa dentro de la carpeta LoRa\n",
    "PATH_ALITERACION = os.path.join(LORA_BASE_PATH, \"lora_aliteracion\")\n",
    "PATH_ANAFORA = os.path.join(LORA_BASE_PATH, \"lora_anafora\")\n",
    "PATH_ANIMALIZACION = os.path.join(LORA_BASE_PATH, \"lora_animalizacion\")\n",
    "PATH_ASINDENTON = os.path.join(LORA_BASE_PATH, \"lora_asindeton\")\n",
    "PATH_COSIFICACION = os.path.join(LORA_BASE_PATH, \"lora_cosificacion\")\n",
    "PATH_EPITETO = os.path.join(LORA_BASE_PATH, \"lora_epiteto\")\n",
    "PATH_HIPERBOLE = os.path.join(LORA_BASE_PATH, \"lora_hiperbole\")\n",
    "PATH_PARALELISMO = os.path.join(LORA_BASE_PATH, \"lora_paralelismo\")\n",
    "PATH_PERSONIFICACION = os.path.join(LORA_BASE_PATH, \"lora_personificacion\")\n",
    "PATH_PLEONASMO = os.path.join(LORA_BASE_PATH, \"lora_pleonasmo\")\n",
    "PATH_POLISINDENTON = os.path.join(LORA_BASE_PATH, \"lora_polisindeton\")\n",
    "PATH_SIMIL = os.path.join(LORA_BASE_PATH, \"lora_simil\")\n",
    "PATH_METAFORA = os.path.join(LORA_BASE_PATH, \"modelo_lora_final\")\n"
   ],
   "id": "2c4f28e05610e9d7",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T03:38:13.952601Z",
     "start_time": "2026-01-01T03:38:07.074696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Cargando Tokenizer desde {BASE_MODEL}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, local_files_only=True)\n",
    "\n",
    "print(f\"Cargando Modelo Base desde {BASE_MODEL} con MPS...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=dtype,\n",
    "    device_map={\"\": \"mps\"},\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"eager\",\n",
    "    local_files_only=True\n",
    ")"
   ],
   "id": "e977dfdee3a7c3a5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando Tokenizer desde /Users/dam/ESCOM/ML/Poemas/models/BASE_MODEL...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando Modelo Base desde /Users/dam/ESCOM/ML/Poemas/models/BASE_MODEL con MPS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.25s/it]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T03:38:38.511091Z",
     "start_time": "2026-01-01T03:38:27.300163Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Cargando Adaptadores LoRA...\")\n",
    "try:\n",
    "    # Cargamos el primer adaptador para inicializar el PeftModel\n",
    "    model = PeftModel.from_pretrained(\n",
    "        model,\n",
    "        PATH_METAFORA,\n",
    "        adapter_name=\"metafora\"\n",
    "    )\n",
    "\n",
    "    # Cargamos el resto de los adaptadores vinculando cada ruta a su nombre\n",
    "    model.load_adapter(PATH_ALITERACION, adapter_name=\"aliteracion\")\n",
    "    model.load_adapter(PATH_ANAFORA, adapter_name=\"anafora\")\n",
    "    model.load_adapter(PATH_ANIMALIZACION, adapter_name=\"animalizacion\")\n",
    "    model.load_adapter(PATH_ASINDENTON, adapter_name=\"asindeton\")\n",
    "    model.load_adapter(PATH_COSIFICACION, adapter_name=\"cosificacion\")\n",
    "    model.load_adapter(PATH_EPITETO, adapter_name=\"epiteto\")\n",
    "    model.load_adapter(PATH_HIPERBOLE, adapter_name=\"hiperbole\")\n",
    "    model.load_adapter(PATH_PARALELISMO, adapter_name=\"paralelismo\")\n",
    "    model.load_adapter(PATH_PERSONIFICACION, adapter_name=\"personificacion\")\n",
    "    model.load_adapter(PATH_PLEONASMO, adapter_name=\"pleonasmo\")\n",
    "    model.load_adapter(PATH_POLISINDENTON, adapter_name=\"polisindeton\")\n",
    "    model.load_adapter(PATH_SIMIL, adapter_name=\"simil\")\n",
    "\n",
    "    model.eval()\n",
    "    print(f\"ADAPTADORES CARGADOS: {list(model.peft_config.keys())}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error al cargar adaptadores: {e}\")\n"
   ],
   "id": "f9e54a655241a870",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando Adaptadores LoRA...\n",
      "ADAPTADORES CARGADOS: ['metafora', 'aliteracion', 'anafora', 'animalizacion', 'asindeton', 'cosificacion', 'epiteto', 'hiperbole', 'paralelismo', 'personificacion', 'pleonasmo', 'polisindeton', 'simil']\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T03:39:55.213567Z",
     "start_time": "2026-01-01T03:39:55.203154Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generar_poema(palabra, figura=\"metafora\"):\n",
    "    model.set_adapter(figura)\n",
    "\n",
    "    prompt = f'Escribe un poema usando la figura retórica \"{figura}\" con la palabra \"{palabra}\".\\nPoema:\\n'\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=80,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            repetition_penalty=1.1,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            use_cache=True\n",
    "        )\n",
    "\n",
    "    resultado = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    poema = resultado.split(\"Poema:\")[-1].strip()\n",
    "\n",
    "    print(f\"\\n[{figura.upper()}] {palabra}:\\n{poema}\\n\" + \"-\" * 30)\n"
   ],
   "id": "8666febd2261d257",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def visualizar_atencion(palabra, figura=\"metafora\", capa=-1, cabeza=0):\n",
    "    model.set_adapter(figura)\n",
    "\n",
    "    prompt = f'Escribe un poema usando la figura retórica \"{figura}\" con la palabra \"{palabra}\".\\nPoema:\\n'\n",
    "    inputs_prompt = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_tokens = model.generate(\n",
    "            **inputs_prompt,\n",
    "            max_new_tokens=40,\n",
    "            do_sample=True,\n",
    "            temperature=0.7\n",
    "        )\n",
    "\n",
    "        outputs = model(output_tokens, output_attentions=True)\n",
    "        attentions = outputs.attentions\n",
    "\n",
    "    attn_matrix = attentions[capa][0, cabeza].detach().cpu().float().numpy()\n",
    "    full_tokens = [tokenizer.decode([t]) for t in output_tokens[0]]\n",
    "\n",
    "    plt.figure(figsize=(14, 12))\n",
    "    sns.heatmap(attn_matrix, xticklabels=full_tokens, yticklabels=full_tokens, cmap='magma')\n",
    "    plt.title(f\"Atención Completa: {figura.upper()} (Prompt + Generación)\")\n",
    "    plt.show()\n"
   ],
   "id": "72f3d4760be9fd82"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T03:49:38.232377Z",
     "start_time": "2026-01-01T03:49:24.540526Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    figuras_a_probar = [\n",
    "        #\"aliteracion\",\n",
    "        #\"anafora\",\n",
    "        \"animalizacion\",\n",
    "        #\"asindeton\",\n",
    "        #\"epiteto\",\n",
    "        #\"hiperbole\",\n",
    "        #\"paralelismo\",\n",
    "        #\"pleonasmo\",\n",
    "        #\"polisindeton\",\n",
    "        #\"simil\",\n",
    "        #\"metafora\",\n",
    "        \"personificacion\",\n",
    "        \"cosificacion\"\n",
    "    ]\n",
    "\n",
    "    palabra_test = \"Burócrata\"\n",
    "\n",
    "    for fig in figuras_a_probar:\n",
    "        try:\n",
    "            generar_poema(palabra_test, fig)\n",
    "        except Exception as e:\n",
    "            print(f\"Error generando con {fig}: {e}\")"
   ],
   "id": "b134bcb26d0b1089",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ANIMALIZACION] Burócrata:\n",
      "la burócrata es un animal lento,\n",
      "con garras de plomo y ojos de neón,\n",
      "su paso es pesado, su mirada es dura,\n",
      "en un mundo que se mueve con velocidad.\n",
      "<|endoftext|>\n",
      "------------------------------\n",
      "\n",
      "[PERSONIFICACION] Burócrata:\n",
      "la burócrata es una lista interminable,\n",
      "de papeles y formularios sin fin,\n",
      "de fechas y horas que se repiten,\n",
      "en un ciclo sin descanso, sin fin.\n",
      "<|endoftext|>\n",
      "------------------------------\n",
      "\n",
      "[COSIFICACION] Burócrata:\n",
      "la burócrata es un peso que carga\n",
      "sobre los hombros de la ciudad\n",
      "un lastre que no se mueve\n",
      "y que nos hace sentir cansados\n",
      "<|endoftext|>\n",
      "------------------------------\n"
     ]
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
